# Proyecto de Ingesta y Procesamiento de Eventos con Kafka, Apache Flink y MongoDB

## ğŸŒŸ Objetivo del Proyecto
Implementar un pipeline de procesamiento de datos en tiempo real utilizando tecnologÃ­as open-source modernas:
- **Kafka** para la ingesta de eventos.
- **Apache Flink** para el procesamiento y transformaciÃ³n de los eventos.
- **MongoDB** para la persistencia final.
- **Flask API** para exponer los datos en tiempo real.

Este flujo se basa en eventos provenientes de la API de TransportAPI (trenes en vivo en Reino Unido).

---

## ğŸ“Š Arquitectura General
```
+-------------+         +-----------+         +-------------+         +---------+         +-----------+
|  Transport  | --> --> |  Producer | ----->  |  Kafka       | ----->  |  Flink  | ----->  |  MongoDB  |
|   API (UK)  |         |  (Python) |         |   Broker     |         | Transform|         |  Database |
+-------------+         +-----------+         +-------------+         +---------+         +-----------+
                                                        
                                                             \                                
                                                              \                              
                                                                \---> Flask API (visualizaciÃ³n)
```

---

## âš™ï¸ TecnologÃ­as Utilizadas
- **Apache Kafka** (Confluent Platform 7.5)
- **Apache Flink 1.17**
- **MongoDB 6.0**
- **Python 3.10**
- **Flask** (API Rest)
- **Docker & Docker Compose**

---

## ğŸ“„ Estructura del Proyecto
```
kafka-project01/
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ flink-jobrunner/
â”‚       â””â”€â”€ Dockerfile
â”œâ”€â”€ jars/
â”‚   â”œâ”€â”€ flink-connector-kafka-1.17.0.jar
â”‚   â””â”€â”€ kafka-clients-3.4.0.jar
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ producer.py
â”‚   â”œâ”€â”€ transform_flink_job.py
â”‚   â”œâ”€â”€ persist_mongo_consumer.py
â”‚   â””â”€â”€ flask_api.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ docker-compose.yml
```

---

## âš¡ InstalaciÃ³n y EjecuciÃ³n Paso a Paso

### 1. Clonar el repositorio
```bash
git clone https://github.com/tu_usuario/kafka-project01.git
cd kafka-project01
```

### 2. Configurar credenciales de TransportAPI
Edita `scripts/producer.py` e inserta tu `APP_ID` y `API_KEY` reales.

### 3. Construir contenedores Docker
```bash
docker-compose build
```

### 4. Levantar toda la infraestructura
```bash
docker-compose up -d
```

### 5. Ejecutar el Producer manualmente (API real)
```bash
python scripts/producer.py
```

### 6. Verificar datos en MongoDB
```bash
docker exec -it mongodb mongosh
use transport_db
db.trains_flink_clean.find().pretty()
```

### 7. Acceder a la API Flask
```
http://localhost:5000/trains
```

---

## ğŸ“ Notas Adicionales
- Puedes cambiar la estaciÃ³n consultada desde la API editando el `station_code` en `producer.py`.
- Flink y el Consumer estÃ¡n dockerizados y corren automÃ¡ticamente.
- La arquitectura es extensible y podrÃ­a incluir dashboards (ej. Power BI o Grafana).

---

## ğŸ“š Aprendizajes Clave
- Conceptos de arquitecturas orientadas a eventos.
- Uso de Kafka como sistema de mensajerÃ­a.
- Transformaciones con Apache Flink.
- Persistencia de datos con MongoDB.
- VisualizaciÃ³n RESTful con Flask.

---

## ğŸŒŸ Autor
Carlos - Ingeniero de Datos

---

## ğŸ“ Futuras Mejoras
- Procesamiento por ventanas de tiempo en Flink.
- VisualizaciÃ³n en tiempo real con dashboards.
- AutenticaciÃ³n y seguridad para la API REST.

